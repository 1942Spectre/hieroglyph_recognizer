{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building the distance calculator function\n",
    "\n",
    "def get_dist(siameese , x1,x2 ): \n",
    "    \n",
    "    x1 = np.asanyarray(Image.fromarray(x1).resize(input_shape))\n",
    "    \n",
    "    x2 = np.asanyarray(Image.fromarray(x2).resize(input_shape))\n",
    "    \n",
    "    x1_output = siameese.predict(x1.reshape((1,x1.shape[0] , x1.shape[1] , x1.shape[2])))\n",
    "    \n",
    "    x2_output = siameese.predict(x2.reshape((1,x2.shape[0] , x2.shape[1] , x2.shape[2])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dist = tf.abs(x1_output-x2_output)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def build_models():\n",
    "    \n",
    "    #### Creating a siameese model from first 8 layers of VGG-19\n",
    "    \n",
    "    \n",
    "    vgg_19 = keras.models.load_model(\"vgg-19.h5\")\n",
    "    \n",
    "    siameese = keras.models.Sequential()\n",
    "\n",
    "    siameese.add(vgg_19.layers[0])\n",
    "    siameese.add(vgg_19.layers[1])\n",
    "    siameese.add(vgg_19.layers[2])\n",
    "    siameese.add(vgg_19.layers[3])\n",
    "    siameese.add(keras.layers.Reshape((56,56,256)))\n",
    "    \n",
    "    #### Building the classisier model\n",
    "    \n",
    "    difference_model = keras.models.Sequential()\n",
    "\n",
    "    difference_model.add(keras.layers.Input(shape = (56,56,256)))\n",
    "\n",
    "    difference_model.add(keras.layers.Flatten())\n",
    "    \n",
    "    difference_model.add(keras.layers.Dense(100 , activation = \"relu\"))\n",
    "    \n",
    "    difference_model.add(keras.layers.Dense(50 , activation = \"relu\"))\n",
    "\n",
    "    difference_model.add(keras.layers.Dense(1 , activation = \"sigmoid\"))\n",
    "    \n",
    "    difference_model.compile(optimizer = \"adam\" , loss = \"binary_crossentropy\" , metrics = [\"accuracy\"])\n",
    "    \n",
    "    return (siameese,difference_model)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_predict(siameese , classifier , img1 , img_2):\n",
    "    \n",
    "    dist = get_dist(siameese , img1,img_2)\n",
    "    \n",
    "    return classifier.predict(dist)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.array(plt.imread(\"EgyptImages/img1.jpg\"))\n",
    "img_2 = np.array(plt.imread(\"EgyptImages/img2.jpg\"))\n",
    "img_1_gen = np.array(img_1)\n",
    "img_2_gen = np.array(img_2)\n",
    "img_1_gen[:,:,1] += 50\n",
    "img_2_gen[:,:,1] += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "siameese,classifier = build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 56, 56, 256)       0         \n",
      "=================================================================\n",
      "Total params: 38,720\n",
      "Trainable params: 38,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siameese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(directory):\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for image in os.listdir(directory):\n",
    "        \n",
    "        images.append(np.array(plt.imread(directory + \"/\" + image)))\n",
    "        \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = get_images(\"EgyptImages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(Images):\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(len(Images)-1):\n",
    "        \n",
    "        temp = np.array(Images[i])\n",
    "        \n",
    "        temp[:,:,1] += 50\n",
    "        \n",
    "        images.append(temp)\n",
    "        \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generate_images(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(images , generated_images):\n",
    "    \n",
    "    trues = []\n",
    "    \n",
    "    for i in range(len(images)-1):\n",
    "        \n",
    "        trues.append(get_dist(siameese , images[i] , generated_images[i] ))\n",
    "    \n",
    "        \n",
    "    falses = []\n",
    "    \n",
    "    for i in range(len(images)-1):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            falses.append(get_dist(siameese , images[i] , generated_images[i+1] ))\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            falses.append(get_dist(siameese , images[i] , images[i-1] ))\n",
    "            \n",
    "    y = [1 for true in trues]\n",
    "    \n",
    "    y.extend([0 for false in falses])\n",
    "    \n",
    "    X = trues\n",
    "    \n",
    "    X.extend(falses)\n",
    "    \n",
    "    return(np.array(X) , np.array(y))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_pairs(Images , generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 85ms/sample - loss: 297.5800 - accuracy: 0.6000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 119172.1641 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 59985.3242 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 14646.2129 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 1555.0842 - accuracy: 0.4000\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 13003.6533 - accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 5285.7183 - accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 18929.0156 - accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 14924.1621 - accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 8356.4971 - accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 4196.4136 - accuracy: 0.6000\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 1121.6907 - accuracy: 0.6000\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 9047.0420 - accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 6237.5947 - accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 4257.2251 - accuracy: 0.6000\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 5837.3989 - accuracy: 0.6000\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 41.2447 - accuracy: 0.7000\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 3380.3140 - accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 40.1676 - accuracy: 0.9000\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 1670.0531 - accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 2206.3196 - accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 250.3193 - accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 243.7501 - accuracy: 0.9000\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 5.6378 - accuracy: 0.9000\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba4c47e048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = X.reshape((10,56,56,256)) , y = y , epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = get_images(\"EgyptImages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_2(images , generated_images):\n",
    "    \n",
    "    trues = []\n",
    "    \n",
    "    for i in range(len(images)-1):\n",
    "        \n",
    "        trues.append(get_dist(siameese , images[i] , generated_images[i] ))\n",
    "    \n",
    "        \n",
    "    falses = []\n",
    "    \n",
    "    for i in range(len(images)-1):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            falses.append(get_dist(siameese , images[i] , generated_images[i+2] ))\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            falses.append(get_dist(siameese , images[i] , images[i-2] ))\n",
    "            \n",
    "    y = [1 for true in trues]\n",
    "    \n",
    "    y.extend([0 for false in falses])\n",
    "    \n",
    "    X = trues\n",
    "    \n",
    "    X.extend(falses)\n",
    "    \n",
    "    return(np.array(X) , np.array(y))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test.reshape((10,56,56,256)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/artemis/anaconda3/envs/gpu-env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: siameese/assets\n"
     ]
    }
   ],
   "source": [
    "siameese.save(\"siameese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier/assets\n"
     ]
    }
   ],
   "source": [
    "classifier.save(\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
